{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.optim as optim, torch.nn as nn\n",
    "import sys, os\n",
    "from collections import defaultdict\n",
    "sys.path.append(\"./..\") # \\todo: change for relative import\n",
    "from dataset.ASMGMovieLens import ASMGMLDataModule\n",
    "from utils.save import get_timestamp, save_as_json, get_path_from_re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from MF.model import get_model\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# filter warning for not setting validation set\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \"Total length of `DataLoader` across ranks is zero.*\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device = }')\n",
    "\n",
    "# get training regime experiment id\n",
    "# timestamp = get_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# control flow parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = dict(\n",
    "    input_path=\"../../data/preprocessed/ml_processed.csv\",\n",
    "    validation_start_period=None,\n",
    "    validation_end_period=None,\n",
    "    test_start_period=25,\n",
    "    test_end_period=31,  # 25\n",
    "    train_window=10,\n",
    "    max_epochs=20,  # 20\n",
    "    batch_size=1024,\n",
    "    learning_rate=1e-3,  # 1e-2 is the ASMG MF implementation\n",
    "    model_filename_stem='first_mf',\n",
    "    seed=6202,\n",
    "    save_model=False,\n",
    "    save_result=True\n",
    ")\n",
    "model_params = dict(\n",
    "    alias=\"MF\",\n",
    "    n_users=43183,\n",
    "    n_items=51149,\n",
    "    n_latents=8,\n",
    "    l2_regularization_constant=1e-6,\n",
    ")\n",
    "train_params[\"model_checkpoint_dir\"] = f'./../../model/{model_params[\"alias\"]}/BM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training components\n",
    "torch.manual_seed(train_params[\"seed\"])\n",
    "model = get_model({**model_params, **train_params})#.to(device)\n",
    "# loss_function = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=train_params[\"learning_rate\"])\n",
    "\n",
    "# progess log\n",
    "progress_bar = TQDMProgressBar(refresh_rate=200)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=False, \n",
    "    min_delta=1e-4)\n",
    "\n",
    "# initialize results container\n",
    "res_dict = defaultdict(lambda: [])\n",
    "\n",
    "# initialize dict of hyperparameters\n",
    "train_hparams = {'learning_rate': train_params[\"learning_rate\"],\n",
    "            'l2_regularization_constant': model_params[\n",
    "                \"l2_regularization_constant\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_params[\"validation_start_period\"] is not None:\n",
    "    \n",
    "    # BU validation regime routine\n",
    "    for val_period in range(\n",
    "            train_params[\"validation_start_period\"], train_params[\"validation_end_period\"] + 1):\n",
    "\n",
    "        # update periods\n",
    "        train_window_begin = val_period - train_params[\"train_window\"]\n",
    "        train_window_end = val_period - 1\n",
    "        print(\n",
    "            f\"train periods: {train_window_begin}-{train_window_end}\",\n",
    "            f\"test period: {val_period}\", sep=\"\\n\")\n",
    "\n",
    "        # make checkpoint dir\n",
    "        model_checkpoint_subdir = f'{train_params[\"model_checkpoint_dir\"]}/' + (\n",
    "            f'/V{val_period:02}')\n",
    "        if not os.path.exists(model_checkpoint_subdir):\n",
    "            os.makedirs(model_checkpoint_subdir)\n",
    "\n",
    "        val_trainer = Trainer(\n",
    "            accelerator=\"auto\", devices=1 if torch.cuda.is_available() else 0,\n",
    "            max_epochs=train_params[\"max_epochs\"], reload_dataloaders_every_n_epochs=1,\n",
    "            enable_checkpointing=train_params[\"save_model\"],\n",
    "            default_root_dir=model_checkpoint_subdir,\n",
    "            callbacks=[\n",
    "                early_stopping, progress_bar]\n",
    "        )\n",
    "        \n",
    "        # load datsets\n",
    "        train_dm = ASMGMLDataModule(\n",
    "            train_params[\"input_path\"], train_params[\"batch_size\"],\n",
    "            train_window_begin, train_window_end, period_val=val_period)\n",
    "\n",
    "        # train\n",
    "        val_trainer.fit(\n",
    "            model, datamodule=train_dm)\n",
    "        print(f\"finished val_period {val_period}\")\n",
    "\n",
    "        # log hyperparameters\n",
    "        val_trainer.logger.log_hyperparams({\n",
    "            **train_hparams,\n",
    "            'n_epochs': val_trainer.current_epoch - early_stopping.patience\n",
    "        }, metrics=early_stopping.best_score)\n",
    "\n",
    "        torch.manual_seed(train_params[\"seed\"])\n",
    "        model.reset_parameters()\n",
    "\n",
    "else:\n",
    "    print(\"skipped validation cycle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_params[\"test_start_period\"] is not None:\n",
    "    \n",
    "    # BU test routine\n",
    "    for test_period in range(\n",
    "        train_params[\"test_start_period\"], train_params[\"test_end_period\"] + 1):\n",
    "\n",
    "        # update periods\n",
    "        train_window_begin = test_period - train_params[\"train_window\"]\n",
    "        train_window_end = test_period - 1 \n",
    "        print(\n",
    "            f\"train periods: {train_window_begin}-{train_window_end}\", \n",
    "            f\"test period: {test_period}\", sep=\"\\n\")\n",
    "\n",
    "        # make checkpoint dir\n",
    "        model_checkpoint_subdir = f'{train_params[\"model_checkpoint_dir\"]}' + (\n",
    "            f'/T{test_period:02}' if train_params[\"save_model\"] else \"\")\n",
    "        if not os.path.exists(model_checkpoint_subdir):\n",
    "            os.makedirs(model_checkpoint_subdir) \n",
    "\n",
    "        # model_checkpoint_path = f'{model_checkpoint_subdir}/' \\\n",
    "        #     f'{train_params[\"model_filename_stem\"]}.pth'\n",
    "\n",
    "        test_trainer = Trainer(\n",
    "                accelerator=\"auto\", devices=1 if torch.cuda.is_available() else 0,\n",
    "                max_epochs=train_params[\"max_epochs\"], reload_dataloaders_every_n_epochs=1,\n",
    "                enable_checkpointing=train_params[\"save_model\"],\n",
    "                default_root_dir=model_checkpoint_subdir,\n",
    "                logger=False,\n",
    "                callbacks=[progress_bar]\n",
    "            )\n",
    "\n",
    "        # load datsets \n",
    "        train_dm = ASMGMLDataModule(\n",
    "            train_params[\"input_path\"], train_params[\"batch_size\"], \n",
    "            train_window_begin, train_window_end)\n",
    "        test_dm = ASMGMLDataModule(\n",
    "            train_params[\"input_path\"], train_params[\"batch_size\"], test_period)\n",
    "\n",
    "        # train\n",
    "        test_trainer.fit(\n",
    "            model, datamodule=train_dm)\n",
    "        res_dict[\"testPeriod\"].append(test_period)\n",
    "        res_dict[\"trainLoss\"].append(test_trainer.logged_metrics[\"train_loss\"].item())\n",
    "        print(f\"finished test_period {test_period}\")\n",
    "\n",
    "        # test\n",
    "        test_trainer.test(model, datamodule=test_dm)\n",
    "        res_dict[\"testLoss\"].append(test_trainer.logged_metrics[\"test_loss\"].item())\n",
    "\n",
    "        torch.manual_seed(train_params[\"seed\"])\n",
    "        model.reset_parameters()\n",
    "        \n",
    "    else:\n",
    "        timestamp = get_timestamp()\n",
    "        df_path = f\"{model_checkpoint_subdir.replace(f'/T{test_period:02}', '')}/{timestamp}.csv\"\n",
    "        df_path\n",
    "\n",
    "        # calculate and display average loss\n",
    "        res_df = pd.DataFrame(res_dict)\n",
    "\n",
    "        average_srs = res_df.mean()\n",
    "        average_srs.at[\"testPeriod\"] = \"mean\"\n",
    "        print(average_srs)\n",
    "\n",
    "        if train_params[\"save_result\"]: \n",
    "            pd.concat((res_df, average_srs.to_frame().T), axis=0, ignore_index=True\n",
    "            ).to_csv(df_path, index=False)\n",
    "            print(f\"saved results csv at: {os.path.abspath(df_path)}\")\n",
    "\n",
    "else:\n",
    "    print(\"skipped test cycle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('alpha')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26ae111bd7481dd6266ac7e84bf867498b6b0fbfa14667d050bcdd9b0494c793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
